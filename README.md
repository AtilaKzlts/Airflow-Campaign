![image](https://github.com/AtilaKzlts/Airflow-Campaign/blob/main/assets/Bar.svg)


<div align="center"> <h1>Campaign Performance Tracking</h1> </p> </div>



## Project Introduction
In this project, a dynamic data pipeline supported by a live dashboard was created to enable the marketing department to track the performance of three different campaigns on a monthly basis starting from January 2025. Since the primary goal of the campaigns was promotion, our main metric was ***impressions***, while key performance indicators such as clicks and Cost Per Click (CPC) were also prioritized as key metrics.

## About the Dataset

| **Column Name**            | **Description**                                          |
|----------------------------|----------------------------------------------------------|
| `campaign_id`               | Unique identifier for each marketing campaign            |
| `campaign_name`             | Name or identifier of the campaign                       |
| `date`                      | Date of the recorded data                                |
| `clicks`                    | Number of clicks generated by the campaign               |
| `impressions`               | Number of times the ad was shown to users                |
| `platform`                  | Advertising platform (e.g., Google Ads, Facebook)        |
| `cost`                      | Total cost of the campaign on the given date (£)       |
| `region`                    | Geographical region of the campaign's performance        |


## Steps

![image](https://github.com/AtilaKzlts/Airflow-Campaign/blob/main/assets/airflowsheets.svg)

**Data Collection;**

+ The necessary data was retrieved from AWS S3 using API calls.
Scheduled to pull the most up-to-date data on a regular basis.

**Data Cleaning and Processing**
+ The raw data was cleaned to remove errors and missing values.
Key metrics, such as Click Through Rate (CTR) and Cost Per Click (CPC), were calculated during this step.
Data was processed using Apache Airflow, ensuring the pipeline was automated and could handle large datasets efficiently.

**Data Storage**
+ Once cleaned and processed, the data was uploaded to Google Sheets via API for easy sharing and further analysis by the marketing team.

**Data Visualization**
+ Tableau was used to create live dashboards that visualized the campaign performance metrics.
This enabled the marketing team to view real-time insights and track the campaigns on a monthly basis.

## DAG structure
![image](https://github.com/AtilaKzlts/Airflow-Campaign/blob/main/assets/diagram.png)

**Data Pull (Pull)**

- **Purpose**: To fetch the data from AWS S3.
- **Method**: The `S3Hook` is used to retrieve the file from S3, which is then converted into a StringIO object and loaded into a Pandas DataFrame.
- **Error Handling**: An error is raised if the data is empty or cannot be fetched.

**Data Transformation (Transform)**

- **Purpose**: To clean the data, add new columns, and perform logical checks.
- **Method**: Missing values in the DataFrame are checked, and logical validations are applied (for example, ensuring "clicks" and "impressions" are not negative). New calculations are added, such as `ctr` (click-through rate) and `cpc` (cost per click).
- **Error Handling**: An `AssertionError` is raised if the data contains invalid values, such as negative numbers.

**Data Load (Load)**

- **Purpose**: To load the transformed data into Google Sheets.
- **Method**: The Google Sheets API is used to write data to Sheets. The relevant credentials are loaded in JSON format for connecting to Google Sheets. The process ensures that old data is cleaned up before new data is uploaded.
- **Error Handling**: An error is raised if the connection to Google Sheets cannot be established or if the data cannot be uploaded.

**DAG Structure and Dependencies**

- **Dependencies**: `extract_task` (data pull) → `transform_task` (data transformation) → `load_task` (data load).
- **Scheduling and Retries**: Each task is configured with 3 retries and a 10-minute interval in case of failure.

This structure ensures that any issues with the data are logged properly, and the task is retried within the specified time intervals.

[See DAG](https://github.com/AtilaKzlts/Airflow-Campaign/blob/main/assets/airflow_script.py)


## Snapshot

![image](https://github.com/AtilaKzlts/Airflow-Campaign/blob/main/assets/Dashboard%201.png) 

![image](https://github.com/AtilaKzlts/Airflow-Campaign/blob/main/assets/Dashboard%202.png) 

[See Dashboard](https://public.tableau.com/app/profile/atilla.kiziltas/viz/airlfow_2/Dashboard1)

### [**Return to Portfolio**](https://github.com/AtilaKzlts/Atilla-Portfolio)
